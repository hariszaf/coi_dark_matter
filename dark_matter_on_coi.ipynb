{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dark_matter_on_coi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariszaf/coi_dark_matter/blob/master/dark_matter_on_coi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZlvLj0_FN59",
        "colab_type": "text"
      },
      "source": [
        "# Dark matter on COI marker gene Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9WZimwg5jhh",
        "colab_type": "text"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFSm7O1p5uCQ",
        "colab_type": "text"
      },
      "source": [
        "* Get as many as possible COI sequences from all the 3 domains of life\n",
        "* Keep consensus sequences for each domain conserving as much diversity of those as possible\n",
        "* Build a single tree for all the consensus sequences\n",
        "* Assign query sequences from in-house data in the reference tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLsDZClGATyt",
        "colab_type": "text"
      },
      "source": [
        "## For this ```.ipynb```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avTd5xRoAYo7",
        "colab_type": "text"
      },
      "source": [
        "This is a Google Colab document; In fact, it's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.\n",
        "\n",
        "Python scripts can be implemented on this document. Let's see an example. Just press the **play** button next to the following block of code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKtY3mwFBbmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "860920cc-2138-4210-f62b-38ae89403648"
      },
      "source": [
        "print(\"Hello friend\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello friend\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnunkX_JB8t9",
        "colab_type": "text"
      },
      "source": [
        "This environment, cannot run any languages though. So as we have quite a few blocks of code in ```bash``` you need to be careful as they **will not be performed** on our doc; you will have to copy - paste them on your environment if you would like to see what they are doing exactly. As an example, if you try to **play** the next block of code, you will get an error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xgRv0YNCg2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "echo \"hello friend\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9W89RPMDaiL",
        "colab_type": "text"
      },
      "source": [
        "To avoid any misunderstandings on the code, in the third line of every block of code, there will be either the \n",
        "```\n",
        " #!/usr/bin/python3.5\n",
        "```  \n",
        "statement, pointing out that this is a Python scirpt, or the \n",
        "```\n",
        " #!/usr/bin/bash\n",
        "```\n",
        "for those that are bash scripts. The python version does not really matter as long as it is Python 3. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW235lDtE0zZ",
        "colab_type": "text"
      },
      "source": [
        "Furthermore, you will see some scripts including on their top lines like this:\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "#SBATCH --partition=batch\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --nodelist=\n",
        "#SBATCH --ntasks-per-node=20\n",
        "```\n",
        "\n",
        "These are the scripts that we ran as jobs on the Zorba cluster; we will call them *sbatch* files. I have these ```#SBATCH``` lines as they are **rather important** especially in case we need parallel computing architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TdPqWOSCr_8",
        "colab_type": "text"
      },
      "source": [
        "You are able to comment, write or remove anything you want. However, as the history of this type of documents is not that reliable for the time being, please if you change something leave a clear message what was it and the reason you changed it! \n",
        "\n",
        "Keep in mind that you are able to get this document as a ```.pdf``` file any time you want in a straight-forward way. You just need to ```File --> Print --> Save as PDF``` and that's it.\n",
        "\n",
        "We can now move to our dark matter investigation! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib3ifu_tN_Sw",
        "colab_type": "text"
      },
      "source": [
        "**Important note**\n",
        "\n",
        "I will try to build a GitHub repositoty as soon as possible, and get all the scripts there. **However**, it is rather hard to include individual commands in a repo. Thus, this ```.ipynb``` file will be also incuded in the repo. You can do that by just clicking ```File --> Save a copy in GitHub```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWV79fLUHb6g",
        "colab_type": "text"
      },
      "source": [
        "## Data & Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6TRJdb2IGj7",
        "colab_type": "text"
      },
      "source": [
        "To build our reference tree, we use sequences of the COI marker gene from the 3 domains of life (Eukaryotes, Bacteria and Archaea). We collected these sequences from [MIDORI](http://reference-midori.info/index.html) database for the Eukaryotes and from the NCBI for Bacteria and Archaea. \n",
        "\n",
        "To get the corresponding sequences for Bacteria and Archaea we run the following block of code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLG0ia7Vt8XX",
        "colab_type": "text"
      },
      "source": [
        "### Bacteria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpd2ymRkHbjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 1 : get_webenv.py\n",
        "\n",
        "#!/usr/bin/python3.5\n",
        "\n",
        "# Bacteria\n",
        "import requests\n",
        "import re\n",
        "\n",
        "db = \"nucleotide\"\n",
        "\n",
        "# In the following query we describe what sequences we need to get. Filters allow to get exactly what we need from the NCBI database\n",
        "query = \"Coi[All Fields] OR Cytochrome c oxidase subunit I[All+Fields] OR CO1[All Fields] AND (Bacteria[Organism] OR Bacteria Latreille+et+al. 1825[Organism]) AND (1[SLEN]:2000[SLEN])) AND (bacteria[filter])\"\n",
        "\n",
        "# Get the WebEnv code - this is different every time you run this; thus we keep in a text file and then feed it accordingly in a next command\n",
        "base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "url = base + \"esearch.fcgi?db=\" + db + \"&term=\" + query + \"&usehistory=y\"\n",
        "response = requests.get(url)\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "#f = open(\"response_file.xml\",\"w+\")           # In the response_file.xml file, you may find the EnvWev value returned from the query we asked \n",
        "#f.write(response.text)\n",
        "#f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFGNCxnpMReW",
        "colab_type": "text"
      },
      "source": [
        "After getting the ```WebEnv``` key. We are able to run the following piece of code that will actually return the sequences we asked for. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7YsJ4BoFIR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 2: download_sequences.py\n",
        "\n",
        "#!/usr/bin/python3.5\n",
        "\n",
        "# As you can see, in the end of the query there is a value for the WebEnv parameter; this needs to change accordingly with the output of the previous piece of code\n",
        "# You need to check the response_file.xml to get the WebEnv value returned and add it to the url query\n",
        "webenv = \"MCID_5f45715d8e3d0d1989309a0d\"\n",
        "url_prefix = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&query_key=1&WebEnv=\"\n",
        "url_suffix = \"&rettype=fasta\"\n",
        "url = url_prefix + webenv + url_suffix\n",
        "\n",
        "# Get the url \n",
        "# This is a quite long query and in case you run it, be aware that it will take some time! \n",
        "seqs = requests.get(url)\n",
        "\n",
        "## Keep the sequences to a file \n",
        "#h = open(\"bacteria_sequences.fasta\",\"w+\")\n",
        "#h.write(seqs.text)\n",
        "#h.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q_5mL36PXYn",
        "colab_type": "text"
      },
      "source": [
        "Now in the ```bacteria_sequences.fasta``` file you can find our sequences. Here is the first one, as an example:\n",
        "\n",
        "```\n",
        ">MT634227.1 Wolbachia pipientis strain PY_KM2017 cytochrome c oxidase subunit I (coxA) gene, partial cds\n",
        "TCACTATATTTAATATGCGTGCAAAAGGCATGTCATTGACCAAGATGCCACTATTTGTTTGGTCTGTTTT\n",
        "ATTAACGTCGTTTATGTTAATTGTTGCCTTACCGGTACTTGCTGGTGCTATAACTATGCTGCTAACTGAT\n",
        "CGTAATATAGGTACTTCCTTTTTTGATCCTGCTGGTGGCGGTGATCCTGTGTTATTTCAACACCTGTTTT\n",
        "GGTTTTTTGGTCATCCAGAAGTTTACATAATTATTTTTCCTGCATTTGGCATCATAAGCCAAGTCGTATC\n",
        "AACTTTTTCCCATAGGCCAGTATTTGGCTATAAAGGAATGGTTTATGCAATGATAGGCATAGCAGCATTT\n",
        "GGCTTTATGGTTTGGGCTCACCATATGTTTACTGTTGGGCTTAGCGAAGATGCTGCTATATTTTTTAGCA\n",
        "CTACAACAATTTTTATTGGCGTTATA\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiJxGD7nHASJ",
        "colab_type": "text"
      },
      "source": [
        "As you can see, there is no taxonomy for the sequence, only the species name. So, the next step is to get the full taxonomy for each sequence. \n",
        "\n",
        "To this end, we first need to get the corresponding NCBI Taxonomy Ids to the sequences we got. \n",
        "\n",
        "Thus we will now use another Entrez command (to get the sequences we used ```efetch``` as you can see in the url in the previous block of code). Using the ```esummary``` command we can get all the information related to our queries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQp2kNj0aY2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 3: download_summaries.py\n",
        "\n",
        "#!/usr/bin/python3.5\n",
        "\n",
        "import requests\n",
        "\n",
        "# You need to check the response_file.xml to get the WebEnv value returned and add it to the url query. As we did for the download_sequences.py\n",
        "webenv = \"MCID_5f458093dc1d7a5897749052\"\n",
        "url_prefix = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=nucleotide&query_key=1&WebEnv=\"\n",
        "url_suffix = \"&version=2.0\"\n",
        "url = url_prefix + webenv + url_suffix\n",
        "\n",
        "# Get the url \n",
        "metadata = requests.get(url)\n",
        "\n",
        "# # Create a file and keep the sequences retrieved\n",
        "# h = open(\"bacteria_summaries.fasta\",\"w+\")\n",
        "# h.write(metadata.text)\n",
        "# h.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab2LC7gjglc8",
        "colab_type": "text"
      },
      "source": [
        "And here is the top of the ```bacteria_summaries.txt``` file we just built. \n",
        "```\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
        "<!DOCTYPE eSummaryResult PUBLIC \"-//NLM//DTD esummary nuccore 20190808//EN\" \"https://eutils.ncbi.nlm.nih.gov/eutils/dtd/20190808/esummary_nuccore.dtd\">\n",
        "<eSummaryResult>\n",
        "<DocumentSummarySet status=\"OK\">\n",
        "<DocumentSummary uid=\"1890528706\">\n",
        "\t<Caption>MT634227</Caption>\n",
        "\t<Title>Wolbachia pipientis strain PY_KM2017 cytochrome c oxidase subunit I (coxA) gene, partial cds</Title>\n",
        "\t<Extra>gi|1890528706|gb|MT634227.1|</Extra>\n",
        "\t<Gi>1890528706</Gi>\n",
        "\t<CreateDate>2020/08/19</CreateDate>\n",
        "\t<UpdateDate>2020/08/19</UpdateDate>\n",
        "\t<Flags>0</Flags>\n",
        "\t<TaxId>955</TaxId>\n",
        "\t<Slen>446</Slen>\n",
        "```\n",
        "\n",
        "In the Title label it is mentioned that this sequence is coming from a *Wolbachia pipientis strain*.\n",
        "\n",
        "As you can see, there is also a label called ```TaxId```. That is the NCBI Taxonomy Id for the species from which this sequence is coming from. \n",
        "\n",
        "So now, if you visit the [NCBI Taxonomy Id database]() and make a query for the Id 955 [here](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi) is what you will get. \n",
        "The 955 hits the *Wolbachia pipientis*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_nZy7WQaXV7",
        "colab_type": "text"
      },
      "source": [
        "Now, we do have a NCBI Id, but this is not a taxonomy. To get this, we will use the [FTP: NCBI Taxonomy](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/) and more specifically the ```new_taxdump.tar.gz``` tarball. In this, as you may see in its corresponding [```taxdump_readme.txt```](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/taxdump_readme.txt) file, there is the ```fullnamelineage.dmp``` file, where the complete lineage of each NCBI Taxonomy Id can be found. \n",
        "\n",
        "So, after downloading this file on Zorba, we are now able to make a match between the TaxId of each sequence we got and its corresponding lineage. \n",
        "\n",
        "We should mention here, that the sequences and the summaries retrieved follow the same order, i.e the first sequence is the ```>MT634227.1 Wolbachia pipientis strain PY_KM2017 cytochrome c oxidase subunit I (coxA) gene, partial cds``` and the first summary is its corresponding:\n",
        "```\t\n",
        "<Caption>MT634227</Caption>\n",
        "<Title>Wolbachia pipientis strain PY_KM2017 cytochrome c oxidase subunit I (coxA) gene, partial cds</Title>\n",
        "....\n",
        "```\n",
        "\n",
        "And here is the code for this part:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWjTfQBDHxi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 4: match_taxonomies.sh\n",
        "\n",
        "#!/usr/bin/bash\n",
        "\n",
        "# First, we keep all the TaxIds from the bacteria_summaries.xml file, by making use of a grep command \n",
        "grep \"TaxId\" bacteria_summaries.xml | sed -e 's/<TaxId>//g ; s/<\\/TaxId>//g ; s/\\t//g' > tax_ids.tsv\n",
        "\n",
        "# Likewise, for the caption of the summaries\n",
        "grep \"Caption\" bacteria_summaries.xml | sed -e 's/<Caption>//g ; s/<\\/Caption>//g ; s/\\t//g' > captions_included_in_summaries.tsv\n",
        "\n",
        "# Now, we need to merge those two files in a 2-column file which will be like this: \n",
        "# MT634227\t955\n",
        "# NZ_RXLZ01000172\t40324\n",
        "# (the caption of the sequence in the first and the NCBI Taxonomy Id in the second column)\n",
        "paste captions_included_in_summaries.tsv tax_ids_included_in_summaries.tsv > caption_taxid_in_summaries.tsv\n",
        "\n",
        "# And you may remove the previous intermidate files captions_included_in_summaries.tsv and tax_ids_included_in_summaries.tsv\n",
        "rm captions_included_in_summaries.tsv tax_ids_included_in_summaries.tsv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1B2Deuq4yFF",
        "colab_type": "text"
      },
      "source": [
        "Now we can write a Python script that will read the ```tax_ids_included_in_summaries.tsv``` file and for the TaxId in each line will find the corresponding taxonomy from the ```fullnamelineage.dmp``` file. As already mentioned, we could have these file on the Drive, however it is not suggested to do so in this case. If you would like to run this you just need to get the input files to your computational environment and replace the paths. I am trying to use as few as possible dependencies. If you do run it, keep in mind that it will take some time! We parse all the taxonomies of NCBI if you consider it! ;) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBUiQfddOMze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 5: match_taxid_to_taxonomy.py \n",
        "\n",
        "#!/usr/bin/python3.5\n",
        "\n",
        "import io\n",
        "\n",
        "# Important note! \n",
        "# The fullnamelineage.dmp we are about to use is a non-ascii file; thus we need to remember forcing for the utf-8 encoding\n",
        "\n",
        "\n",
        "# Files we need\n",
        "parent_directory = \"/home1/haris/Desktop/stsm/sequences_of_life/bacteria\"\n",
        "ncbi_taxonomy_fullname_file = parent_directory + \"/ncbi_taxonomy/fullnamelineage.dmp\"\n",
        "caption_taxids_in_summaries_file = parent_directory + \"/caption_taxid_in_summaries.tsv\"\n",
        "\n",
        "\n",
        "# Build a dictionary with the information included in the fullnamelineage.dmp file in a way that suits for our case\n",
        "\n",
        "dict_ncbi_tax_id_full_taxonomy = {}\n",
        "\n",
        "with io.open(ncbi_taxonomy_fullname_file, mode=\"r\", encoding=\"utf-8\") as tax_file:\n",
        "\n",
        "    for entry in tax_file:\n",
        "\n",
        "\n",
        "        elements = entry.split(\"\\t\")\n",
        "\n",
        "        ncbi_tax_id = elements[0]\n",
        "        species_name = elements[2]\n",
        "        lineage = elements[4]\n",
        "\n",
        "        # Process these elements to convert them in a format that suits to the needs of our study; see further on the text\n",
        "        lineage = lineage.replace('cellular organisms; ','')\n",
        "        lineage = lineage.replace('; ',';')\n",
        "        lineage = lineage.replace(' ','_')\n",
        "\n",
        "        species_name = species_name.replace(' ','_')\n",
        "\n",
        "        # Build the full taxonomy of the species\n",
        "        taxonomy = lineage + species_name\n",
        "\n",
        "        # Keep this info in the corresponding dictiontary\n",
        "        dict_ncbi_tax_id_full_taxonomy[ncbi_tax_id] = taxonomy\n",
        "\n",
        "# Parse the file with both the caption and their corresponding TaxIds and add a column with the full taxonomy they match \n",
        "# using the NCBI TaxId to do the matching\n",
        "with open(caption_taxids_in_summaries_file) as f:\n",
        "\n",
        "    our_taxa = {}\n",
        "\n",
        "    for entry in f:\n",
        "\n",
        "        elements = entry.split(\"\\t\")\n",
        "\n",
        "        # As there is a new line character added in the ncbi id we have not to keep it\n",
        "        entry_ncbi_id = elements[1][:-1]\n",
        "        entry_caption = elements[0]\n",
        "        our_taxa[entry_caption] = entry_ncbi_id\n",
        "\n",
        "    # Parse the dictionary we built from the fullnamelineage.dmp file\n",
        "    for ncbi_id, taxonomy in dict_ncbi_tax_id_full_taxonomy.items():\n",
        "\n",
        "        # And check whether the ncbi id we have mathes with one from our taxa\n",
        "        for caption, entry_ncbi_id in our_taxa.items():\n",
        "\n",
        "            if ncbi_id == entry_ncbi_id:\n",
        "\n",
        "                new_line = caption + \"\\t\" + ncbi_id + \"\\t\" + taxonomy + \"\\n\"\n",
        "                with open(\"caption_taxid_taxonomy.tsv\", 'a+') as output:\n",
        "                    output.write(new_line)\n",
        "                    output.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68D37QYGOLpy",
        "colab_type": "text"
      },
      "source": [
        "Now we are quite happy! Let's have a look in the last file we built, called ```caption_taxid_taxonomies.tsv```.\n",
        "\n",
        "```\n",
        "MG987635\t2488906\tBacteria;Proteobacteria;Alphaproteobacteria;Rickettsiales;Anaplasmataceae;Wolbachieae;Wolbachia;unclassified_Wolbachia;Wolbachia_endosymbiont_of_Chrysolina_herbacea\n",
        "LR798864\t2736843\tBacteria;Proteobacteria;Alphaproteobacteria;Rickettsiales;Rickettsiaceae;Rickettsieae;Rickettsia;unclassified_Rickettsia;Rickettsia_endosymbiont_of_Hemiptera_sp.\n",
        "```\n",
        "As you can see it is a 3 column file, having the caption of each of the sequences retrieved in the first column, its corresponding NCBI Id in the second and its full taxonomy in the third one. Notice the format of the taxonomy. It is rather important for our next steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-akn1GlQwkqH",
        "colab_type": "text"
      },
      "source": [
        "As in Biology, we are far away from best practices, our queries in NCBI is almost sure that will return things we would not like to and loose some others that we would like to have. For the latter we cannot do much, but we can remove the first ones. In our study we have an *incredible* example of such. **Semi-automated** overview of our dataset allowed us to find this phenomenon entry: [*Uncultured bacterium clone CO1 16S ribosomal RNA gene, partial sequence*](https://www.ncbi.nlm.nih.gov/nuccore/GU946143.1). Obviously this is a Bacterial sequence but not from the COI marker gene.. We need to get such cases and remove them; commands like the following allow us to get at least most of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWQVAE4DwiPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " more bacteria_sequences.fasta | grep \">\" | grep 16S | sed 's/>//g ; s/\\.1//g' | awk -F \" \" '{print $1}' > non_coi_captions.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuujXl8gerah",
        "colab_type": "text"
      },
      "source": [
        "So, now we can change our sequence file accordingly, by replacing their title with the corresponding taxonomy, using the caption field. To do this we run a small Python script "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbTtA623e296",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 6: build_seq_with_taxonomies.py\n",
        "\n",
        "#!/usr/bin/python3.5\n",
        "\n",
        "bacteria_path = \"/home1/haris/Desktop/stsm/sequences_of_life/bacteria\"\n",
        "sequences_file = bacteria_path + \"/bacteria_sequences.fasta\"\n",
        "caption_taxid_taxonomy_file = bacteria_path + \"/caption_taxid_taxonomy.tsv\"\n",
        "output_file = bacteria_path + \"/bacteria_sequences_taxonomy.fasta\"\n",
        "banned_captions = bacteria_path + \"/non_coi_captions.tsv\"\n",
        "\n",
        "# Open the file with the sequences retrieved\n",
        "with open(sequences_file, 'r') as seq_file:\n",
        "    \n",
        "    # And the 3 - column file\n",
        "    with open(caption_taxid_taxonomy_file, 'r') as tax_file:\n",
        " \n",
        "        # Build a dictionary with caption : taxonomy pairs    \n",
        "        caption_taxonomy = {}\n",
        "        for line in tax_file:\n",
        "            line = line.split(\"\\t\")\n",
        "            caption = line[0]\n",
        "            taxonomy = line[2]            \n",
        "            caption_taxonomy[caption] = taxonomy\n",
        "\n",
        "        tax_file.close()\n",
        "\n",
        "        # Open the output file \n",
        "        with open(output_file, \"w+\") as output:\n",
        "\n",
        "            # And the file with the non COI sequences' captions; keep those in a list  \n",
        "            with open(banned_captions, \"r\") as banned:\n",
        "                banned_captions = []\n",
        "                for entry in banned:\n",
        "                    banned_captions.append(entry[:-1])\n",
        "                banned.close()\n",
        "                \n",
        "            # Parse the sequence file to match\n",
        "            checking_var = 1\n",
        "            for line in seq_file:\n",
        "                \n",
        "                if line[0] == \">\" :\n",
        "                    \n",
        "                    checking_var = 1\n",
        "                    line = line.split(\" \")\n",
        "                    caption = line[0][1:-2]\n",
        "                    \n",
        "                    # Check if the caption of the study is among the banned ones\n",
        "                    if caption not in banned_captions:\n",
        "\n",
        "                        new_line = \">\" + caption + \" \" + caption_taxonomy[caption]\n",
        "                        output.write(new_line)\n",
        "\n",
        "                    # If it is change the checking_var to 0; this will allow us not to print the sequence lines of this entry to the new file. \n",
        "                    else:\n",
        "                        checking_var = 0\n",
        "\n",
        "                else:\n",
        "                    \n",
        "                    if checking_var == 1:\n",
        "                        output.write(line)\n",
        "                    else:\n",
        "                        continue\n",
        "seq_file.close()\n",
        "output.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1BizIOd4yZb",
        "colab_type": "text"
      },
      "source": [
        "Now we are (almost) ready for the **[PhAT](https://academic.oup.com/bioinformatics/article/35/7/1151/5088318)** algorithm!\n",
        "\n",
        "Before that though, let's see what exactly this step is for. \n",
        "The sequences we got from NCBI which are *supposed* to be from COI of Bacteria are 6.891. If we keep this number of sequences and add to this the number of COI sequences for Eukaryotes that we are about to get in the next step, we will have a huge number of sequences; in this case a phylogeny tree will be almost both impossible to implement and rather hard to comprehend. \n",
        "\n",
        "Thus, we need to reduce the number of sequences, keeping somehow as diversity as possible; i.e building consensus sequences. \n",
        "\n",
        "The PhAT algorithm makes use of the entropy of the sequences and help us on that. However, we said almost ready because the PhAT algorithm needs a MSA (Multiple Sequence Alignment) to run. Thus, first we need to run [```mafft```](https://mafft.cbrc.jp/alignment/software/algorithms/algorithms.html), a great algorithm for alignment. \n",
        "\n",
        "Here is the first sbatch file of our pipeline. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYzUAxDC3IbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 7.1: mafft_job.sh\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "#SBATCH --partition=batch\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --nodelist=\n",
        "#SBATCH --ntasks-per-node=20\n",
        "#SBATCH --mem=\n",
        "# Memory per node specification is in MB. It is optional.\n",
        "# The default limit is 3000MB per core.\n",
        "#SBATCH --job-name=\"bact-mafft\"\n",
        "#SBATCH --output=bacteria_mafft.output\n",
        "#SBATCH --mail-user=haris.zafr@gmail.com\n",
        "#SBATCH --mail-type=ALL\n",
        "#SBATCH --requeue\n",
        "\n",
        "/usr/bin/mafft --thread 20 --globalpair bacteria_sequences_taxonomy.fasta > aligned_bacteria_sequences.fasta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eTiDEipe2bd",
        "colab_type": "text"
      },
      "source": [
        "That is what we did in the first place. However, as Chistina pointed out, we had to deal with a quite important issue; as NCBI neither has a certain policy for including plus or minus sequences, nor says somewhere whether a sequence is one or the other, we have a dataset that most likely includes both. That is bad news for an alignment. So, after reading almost everything about the ```mafft``` tool, we found out about [this](https://mafft.cbrc.jp/alignment/software/adjustdirection.html?fbclid=IwAR0pM2hzZZiCBhdcOJkZP9MmZfPNvnDrPVyDC-OdRfhkEWAGyLl8sJYNbzI) super feauture! \n",
        "\n",
        "So, as these lines are put together, we are running a second alignment for the Bacteria COI sequences with the following sbatch script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEtbFGSGC7rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 7.2: mafft_job_orientation.sh\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "#SBATCH -N 2\n",
        "#SBATCH -B 2:10:1\n",
        "#SBATCH --partition=batch\n",
        "\n",
        "#SBATCH --ntasks-per-node=15\n",
        "#SBATCH --cpus-per-task=1\n",
        "#SBATCH --hint=compute_bound\n",
        "#SBATCH --job-name=\"orient\"\n",
        "\n",
        "#SBATCH --output=bacteria_mafft_orient.output\n",
        "#SBATCH --mail-user=haris.zafr@gmail.com\n",
        "#SBATCH --mail-type=ALL\n",
        "#SBATCH --requeue\n",
        "\n",
        "\n",
        "export MAFFT_N_THREADS_PER_PROCESS=\"1\"\n",
        "export MAFFT_MPIRUN=\"/usr/bin/mpirun -n 30 -npernode 15 -bind-to none\"\n",
        "\n",
        "/mnt/big/Tools/mafft-7.453-with-extensions/bin/mafft --mpi --thread 15 --globalpair --adjustdirectionaccurately \\\n",
        "bacteria_sequences_taxonomy.fasta > aligned_bacteria_sequences_true_orientation.fasta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e0oG_grC737",
        "colab_type": "text"
      },
      "source": [
        "So let us have a look to our two alignments.\n",
        "First, in the one that does **not** take into account the orientation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRAtJDOn55FI",
        "colab_type": "text"
      },
      "source": [
        "![coi_bacteria_alignment.png](https://i.ibb.co/FnjTfHz/coi-bacteria-alignment.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9yW37Ke55qp",
        "colab_type": "text"
      },
      "source": [
        "And now, on the one that derives from the mafft run with the ```--adjustdirectionaccurately``` parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSbfIbyF-2-C",
        "colab_type": "text"
      },
      "source": [
        "![coi_bacteria_alignment_oriented.png](https://i.ibb.co/XSs3CV4/coi-bacteria-alignment-oriented.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wZLoJkq_IWH",
        "colab_type": "text"
      },
      "source": [
        "As you can see, in the second figure, we have a better (not perfect as it would not be possible for multiple reasons) result. It is my belief that we should use this alignment for our next steps; I do not have a great experience on this so any feedback would be more than welcome! :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiYBvqQdJoCp",
        "colab_type": "text"
      },
      "source": [
        "Once the alignment is complete, we are finally ready to run the Phat algorithm. \n",
        "To do so, we need two input files, the alignmnet we built and another one including just the taxonomies of the species included in our MSA. To get this file is quite easy thanks to an ```awk``` command.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6tjImj3Nc_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "more aligned_bacteria_sequences_true_orientation.fasta | grep \">\" | awk -F \" \" '{print $2}' > taxonomies_for_phat.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un8Idw1TJ3S3",
        "colab_type": "text"
      },
      "source": [
        "Now that we have both the input files needed, we may (finally!) run the PhAT alogithm! \n",
        "\n",
        "As we discussed with Laura, we are about to use the **order** level for Eukaryotes to build our consensus sequences. So, I think that it is better to follow this in the case of Bacteria as well. However, please if you have any thoughts on that, share it **before running our trees**.\n",
        "\n",
        "To do this trick, we use the ```--min-tax-level``` parameter. You may find more about this on its [GitHub repo](https://github.com/lczech/gappa/wiki/Subcommand:-phat).\n",
        "\n",
        "Here is the sbatch file to run the phat algorithm - this step is very fast so you may run it even in a personal computer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4GcK6n1NuBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Script No 9: phat_job.sh \n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "#SBATCH --partition=batch\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --nodelist=\n",
        "#SBATCH --ntasks-per-node=20\n",
        "#SBATCH --mem=\n",
        "# Memory per node specification is in MB. It is optional.\n",
        "# The default limit is 3000MB per core.\n",
        "#SBATCH --job-name=\"phat\"\n",
        "#SBATCH --output=phat.output\n",
        "#SBATCH --mail-user=haris.zafr@gmail.com\n",
        "#SBATCH --mail-type=ALL\n",
        "#SBATCH --requeue\n",
        "\n",
        "\n",
        "/home1/haris/Desktop/stsm/sequences_of_life/gappa/bin/gappa prepare phat --taxonomy-file taxonomies_for_phat.tsv \\\n",
        "--sequence-file aligned_bacteria_sequences_true_orientation.fasta \\\n",
        "--target-size 100 \\\n",
        "--min-tax-level 3 \\\n",
        "--out-dir /home1/haris/Desktop/stsm/sequences_of_life/bacteria/phat_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9JPbG0eW8HJ",
        "colab_type": "text"
      },
      "source": [
        "The number of different orders in our Bacteria sequences are about 68.\n",
        "We ran the previous script, with multiple values of ```--target-size``` to \"investigate\" how the algorithm corresponds. \n",
        "\n",
        "So, when we asked for ```--target-size``` equal to 50 and ```--min-tax-level``` 3, then it returned 68 consensus sequences; exactly one consensus for each order, ignoring actually the 50 sequnces asked. \n",
        "\n",
        "Finally, in terms of having at least a 10% of bacterial sequences in our final, unified dataset (the one including both Bacteria, Euakryotes and Archaea) we asked PhAT for ```--target-size``` 100 and ```min-tax-level``` 3; PhAT returned 99 consensus sequences which we will use for our next steps. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZVs5onNY-2j",
        "colab_type": "text"
      },
      "source": [
        "**Important note**\n",
        "\n",
        "Our initial bacterial sequences are ~7.000. However, the number of different species including in those is rather small. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4P-_b_mC8y9",
        "colab_type": "text"
      },
      "source": [
        "## Eukaryotes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0hH-Wq3DAYs",
        "colab_type": "text"
      },
      "source": [
        "For the case of COI things were more straight forward to get the sequences, twice hard to end up with a set of consensus though; as the computational power and time was quite higher. \n",
        "\n",
        "We downloaded the [MIDORI](https://academic.oup.com/bioinformatics/article/34/21/3753/5033384) database; you may find its site [here](http://www.reference-midori.info/). \n",
        "\n",
        "Up to now, we ran our tests on a previous version of MIDORI. However, today (2020.08.30) I found out about the latest version of MIDORI which is based on the lateste [GenBank release](https://www.ncbi.nlm.nih.gov/genbank/release/current/) (2020.08.15) and which includes not only metazoa but all eukaryote sequnece! \n",
        "\n",
        "See here the READEME of the latest MIDORI version:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjSeUwyK30H",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "____________________________________________________________________________\n",
        "\n",
        "MIDORI Reference: \n",
        "\n",
        "The files in this directory are MIDORI reference files created based on GanBank flat files downloaded from ftp://ftp.ncbi.nlm.nih.gov/genbank. \n",
        "Currently, we have five formats those including Mothur, Qiime, RDP Classifire, SINTAX, and SPINGO. \n",
        "We also provide “RAW” files which contain full taxonomy. \n",
        "\n",
        "Since the version GB 237, we include not only Metazoan, but also all Eukaryote sequence in our reference. \n",
        "\n",
        "UNIQ: \n",
        "The UNIQ files contains all unique haplotyps associated with each species. \n",
        "\n",
        "LONGEST: \n",
        "The LONGEST files contains the longest sequence for each species. \n",
        "\n",
        "You will find number after taxonomy. Those numbers represents GenBank Taxonomy ID. \n",
        "We inserted those numbers to differentiate synonyms (ex. Phylum: Ctenophora = Ctenophora_10197, Diatom: Ctenophora = Ctenophora_1003038, flies: Ctenophora = Ctenophora_1803). \n",
        "\n",
        "In the RDP format, we have inserted lacking taxonomy by creating it from lower taxonomic ranking (ex. description in class-level was missing, so it was created from order-level in the following example, >JF502242.1.7041.7724 root_1;Eukaryota_2759;Chordata_7711;class_Crocodylia_1294634;Crocodylia_1294634;Crocodylidae_8493;Crocodylus_8500;Crocodylus intermedius_184240). \n",
        "\n",
        "List files are listing accession numbers collapsed into each sequence both for uniq and longest files. \n",
        "\n",
        "For more information, please take look our manuscript: www.nature.com/articles/sdata201727, or contact us. \n",
        "\n",
        "MIDORI Team \n",
        "____________________________________________________________________________\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrUQUnJnK_AA",
        "colab_type": "text"
      },
      "source": [
        "As we already have set a pipeline for the pre-process of MIDORI sequence, I think it is better for our investigation to **replace** our current MIDORI version with the new one! As we want to see the dark matter on the COI, I think having more taxonomic groups, i.e. Fungi, will be even better! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQAQE9zXC_mm",
        "colab_type": "text"
      },
      "source": [
        "![midori_alignment.png](https://i.ibb.co/kQRbv9H/midori-alignment.png)"
      ]
    }
  ]
}